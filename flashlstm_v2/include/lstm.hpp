#pragma once

#include <cstddef>
#include <cuda_fp16.h>
#include <cuda_runtime_api.h>

namespace flstm {

/**
 * Forward pass for a single-layer LSTM operating entirely on CUDA buffers.
 *
 * Inputs:
 *   - x_tensor_host:     (T, B, I) in pinned host memory (__half)
 *   - h0_device / c0_device: initial states on device (__half, shape B x H)
 *   - weights_ih / weights_hh: FP32 weights on device (4H x I / 4H x H)
 *   - bias_ih / bias_hh: FP32 biases on device (4H)
 *
 * Outputs:
 *   - y_tensor_host:     (T, B, H) in pinned host memory (__half)
 *   - z_cache_device:    (I+H, T*B) column-major FP16 cache written on device
 *   - h_cache_device:    (T+1, B, H) row-major FP16 cache written on device
 *   - c_cache_device:    (T+1, B, H) row-major FP16 cache written on device
 *   - gate_cache_device: (T, B, 4H) row-major FP16 cache written on device
 *   - compute_stream / h2d_stream / d2h_stream: distinct CUDA streams used for
 *         GEMMs, host→device transfers, and device→host transfers respectively.
 *         All three stream handles must be different to enable overlap.
 */
void StreamingLstmForward(
    size_t time_steps,
    size_t batch_size,
    size_t input_size,
    size_t hidden_size,

    const __half *x_tensor_host,
    const __half *h0_device,
    const __half *c0_device,

    const float *weights_ih,
    const float *weights_hh,
    const float *bias_ih,
    const float *bias_hh,

    __half *y_tensor_host,

    __half *z_cache_device,
    __half *h_cache_device,
    __half *c_cache_device,
    __half *gate_cache_device,

    cudaStream_t compute_stream,
    cudaStream_t h2d_stream,
    cudaStream_t d2h_stream
);

/**
 * Backward pass consuming caches generated by StreamingLstmForward.
 *
 * Inputs:
 *   - z_cache_device:   (I+H, T*B) column-major concatenated inputs (FP16)
 *   - h_cache_device:   (T+1, B, H) row-major hidden states (including t=0, FP16)
 *   - c_cache_device:   (T+1, B, H) row-major cell states (including t=0, FP16)
 *   - gate_cache_device:(T, B, 4H) row-major gate activations (FP16)
 *   - dY_tensor_host:   upstream grads w.r.t outputs in host half precision
 *   - d_hn_device / d_cn_device: grads for final states (nullable)
 *   - weights_ih / weights_hh: forward weights (FP32) reused for GEMMs
 *
 * Outputs:
 *   - dx_tensor_host:   gradients w.r.t. inputs written in host half precision
 *   - dW_ih / dW_hh / db_ih / db_hh: parameter gradients (FP32, device)
 *   - dh0_out / dc0_out: gradients for initial states (FP32, device)
 */
void StreamingLstmBackward(
    size_t time_steps,
    size_t batch_size,
    size_t input_size,
    size_t hidden_size,

    const __half *z_cache_device,
    const __half *h_cache_device,
    const __half *c_cache_device,
    const __half *gate_cache_device,

    const __half *dY_tensor_host,
    const __half *d_hn_device,
    const __half *d_cn_device,

    const float *weights_ih,
    const float *weights_hh,

    __half *dx_tensor_host,
    float *dW_ih,
    float *dW_hh,
    float *db_ih,
    float *db_hh,
    float *dh0_out,
    float *dc0_out,

    cudaStream_t stream
);

} // namespace flstm

extern "C" {

void flstm_StreamingLstmForward(
    size_t time_steps,
    size_t batch_size,
    size_t input_size,
    size_t hidden_size,

    const __half *x_tensor_host,
    const __half *h0_device,
    const __half *c0_device,

    const float *weights_ih,
    const float *weights_hh,
    const float *bias_ih,
    const float *bias_hh,

    __half *y_tensor_host,

    __half *z_cache_device,
    __half *h_cache_device,
    __half *c_cache_device,
    __half *gate_cache_device,

    cudaStream_t compute_stream,
    cudaStream_t h2d_stream,
    cudaStream_t d2h_stream
);

void flstm_StreamingLstmBackward(
    size_t time_steps,
    size_t batch_size,
    size_t input_size,
    size_t hidden_size,

    const __half *z_cache_device,
    const __half *h_cache_device,
    const __half *c_cache_device,
    const __half *gate_cache_device,

    const __half *dY_tensor_host,
    const __half *d_hn_device,
    const __half *d_cn_device,

    const float *weights_ih,
    const float *weights_hh,

    __half *dx_tensor_host,
    float *dW_ih,
    float *dW_hh,
    float *db_ih,
    float *db_hh,
    float *dh0_out,
    float *dc0_out,

    cudaStream_t stream
);

} // extern "C"
