#pragma once

#include <cstddef>
#include <cuda_fp16.h>
#include <cuda_runtime_api.h>

namespace flstm {

/**
 * Forward pass for a single-layer LSTM operating entirely on CUDA buffers.
 *
 * Inputs:
 *   - x_tensor_host:     (T, B, I) in pinned host memory (__half)
 *   - h0_device / c0_device: initial states on device (__half, shape B x H)
 *   - weights_ih / weights_hh: FP32 weights on device (4H x I / 4H x H)
 *   - bias_ih / bias_hh: FP32 biases on device (4H)
 *
 * Outputs:
 *   - y_tensor_host:     (T, B, H) in pinned host memory (__half)
 *   - gate_cache_host:   checkpoint buffer storing (⌈T / R⌉, 2, B, H) FP16
 *                        states (h and c) where R is `recompute_interval`
 *                        controlling how frequently backward checkpoints are
 *                        materialised to host.
 *   - compute_stream / h2d_stream / d2h_stream: distinct CUDA streams used for
 *         GEMMs, host→device transfers, and device→host transfers respectively.
 *         All three stream handles must be different to enable overlap.
 */
void StreamingLstmForward(
    size_t time_steps,
    size_t batch_size,
    size_t input_size,
    size_t hidden_size,
    size_t recompute_interval,

    const __half *x_tensor_host,
    const __half *h0_device,
    const __half *c0_device,

    const float *weights_ih,
    const float *weights_hh,
    const float *bias_ih,
    const float *bias_hh,

    __half *y_tensor_host,

    __half *gate_cache_host,
    __half *hy_device,
    __half *cy_device,

    cudaStream_t compute_stream,
    cudaStream_t h2d_stream,
    cudaStream_t d2h_stream
);

/**
 * Backward pass consuming caches generated by StreamingLstmForward.
 *
 * Inputs:
 *   - x_tensor_host:    (T, B, I) inputs in pinned host memory (__half)
 *   - gate_cache_host:  checkpoint buffer storing (⌈T / R⌉, 2, B, H) FP16
 *                       states (h and c) captured every `recompute_interval`
 *                       steps to seed backward recomputation.
 *   - dY_tensor_host:   upstream grads w.r.t outputs in host half precision
 *   - d_hn_device / d_cn_device: grads for final states (nullable)
 *   - c0_device:        initial cell state provided in half precision (B, H)
 *   - weights_ih / weights_hh: forward weights (FP32) reused for GEMMs
 *   - bias_ih / bias_hh: fused bias terms reused during recomputation
 *
 * Outputs:
 *   - dx_tensor_host:   gradients w.r.t. inputs written in host half precision
 *   - dW_ih / dW_hh / db_ih / db_hh: parameter gradients (FP32, device)
 *   - dh0_out / dc0_out: gradients for initial states (FP32, device)
 *   - compute_stream / h2d_stream / d2h_stream: distinct CUDA streams used for
 *         GEMMs and kernels, host→device transfers, and device→host transfers
 *         respectively. All three stream handles must be different to enable
 *         full-overlap streaming behaviour.
 */
void StreamingLstmBackward(
    size_t time_steps,
    size_t batch_size,
    size_t input_size,
    size_t hidden_size,
    size_t recompute_interval,

    const __half *x_tensor_host,
    const __half *y_tensor_host,
    const __half *gate_cache_host,

    const __half *dY_tensor_host,
    const __half *d_hn_device,
    const __half *d_cn_device,
    const __half *h0_device,
    const __half *c0_device,

    const float *weights_ih,
    const float *weights_hh,
    const float *bias_ih,
    const float *bias_hh,

    __half *dx_tensor_host,
    float *dW_ih,
    float *dW_hh,
    float *db_ih,
    float *db_hh,
    float *dh0_out,
    float *dc0_out,

    cudaStream_t compute_stream,
    cudaStream_t h2d_stream,
    cudaStream_t d2h_stream
);

} // namespace flstm

extern "C" {

void flstm_StreamingLstmForward(
    size_t time_steps,
    size_t batch_size,
    size_t input_size,
    size_t hidden_size,
    size_t recompute_interval,

    const __half *x_tensor_host,
    const __half *h0_device,
    const __half *c0_device,

    const float *weights_ih,
    const float *weights_hh,
    const float *bias_ih,
    const float *bias_hh,

    __half *y_tensor_host,

    __half *gate_cache_host,
    __half *hy_device,
    __half *cy_device,

    cudaStream_t compute_stream,
    cudaStream_t h2d_stream,
    cudaStream_t d2h_stream
);

void flstm_StreamingLstmBackward(
    size_t time_steps,
    size_t batch_size,
    size_t input_size,
    size_t hidden_size,
    size_t recompute_interval,

    const __half *x_tensor_host,
    const __half *y_tensor_host,
    const __half *gate_cache_host,

    const __half *dY_tensor_host,
    const __half *d_hn_device,
    const __half *d_cn_device,
    const __half *h0_device,
    const __half *c0_device,

    const float *weights_ih,
    const float *weights_hh,
    const float *bias_ih,
    const float *bias_hh,

    __half *dx_tensor_host,
    float *dW_ih,
    float *dW_hh,
    float *db_ih,
    float *db_hh,
    float *dh0_out,
    float *dc0_out,

    cudaStream_t compute_stream,
    cudaStream_t h2d_stream,
    cudaStream_t d2h_stream
);

} // extern "C"
